{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3134515,"sourceType":"datasetVersion","datasetId":1909705}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define the paths to your dataset directories\ntrain_dir = \"/kaggle/input/deepfake-and-real-images/Dataset/Train\"\nvalidation_dir = \"/kaggle/input/deepfake-and-real-images/Dataset/Validation\"\ntest_dir = \"/kaggle/input/deepfake-and-real-images/Dataset\"\n\n# Define image data generators for training, validation, and test sets\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Set batch size\nbatch_size = 64\n\n# Load and preprocess the training dataset\ntrain_data_v2M = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(260, 260),  # Xception requires input size of 299x299 , efficientnet- 260x260\n    batch_size=batch_size,\n    class_mode='binary'  # Set class_mode according to your dataset\n)\ntrain_data_x = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(299, 299),  # Xception requires input size of 299x299 , efficientnet- 260x260\n    batch_size=batch_size,\n    class_mode='binary'  # Set class_mode according to your dataset\n)\n\n# Load and preprocess the validation dataset\nvalidation_data_v2M = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(260, 260),\n    batch_size=batch_size,\n    class_mode='binary'\n)\nvalidation_data_x = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(299, 299),\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\n\n# Load and preprocess the test dataset\ntest_data_v2M = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(260, 260),\n    batch_size=batch_size,\n    class_mode='binary'\n)\ntest_data_x = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(299, 299),\n    batch_size=batch_size,\n    class_mode='binary'\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T08:21:10.981049Z","iopub.execute_input":"2024-05-04T08:21:10.981475Z","iopub.status.idle":"2024-05-04T08:24:12.061575Z","shell.execute_reply.started":"2024-05-04T08:21:10.981424Z","shell.execute_reply":"2024-05-04T08:24:12.060557Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Found 140002 images belonging to 2 classes.\nFound 140002 images belonging to 2 classes.\nFound 39428 images belonging to 2 classes.\nFound 39428 images belonging to 2 classes.\nFound 190335 images belonging to 3 classes.\nFound 190335 images belonging to 3 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Xception","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.applications import Xception\nfrom keras.optimizers import Adam\nimport torch\n\n# Load the pre-trained Xception model without the top layers\nbase_model = Xception(weights='imagenet', input_shape=(299, 299, 3), include_top=False)\n\n# Freeze the layers of the pre-trained model\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add a new output layer for binary classification\nx = base_model.output\nx = keras.layers.GlobalAveragePooling2D()(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)  # 2 classes for binary classification\n\n# Create the fine-tuned model\nmodel = keras.models.Model(inputs=base_model.input, outputs=outputs)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nwith tf.device('/GPU:0'):\n    history = model.fit(train_data_x, epochs=10, validation_data=validation_data_x)\n\n# Evaluate the model on the test set\nwith tf.device('/GPU:0'):\n    loss, accuracy = model.evaluate(test_data_x)\n    print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T08:24:12.063537Z","iopub.execute_input":"2024-05-04T08:24:12.063853Z","iopub.status.idle":"2024-05-04T10:34:12.193725Z","shell.execute_reply.started":"2024-05-04T08:24:12.063828Z","shell.execute_reply":"2024-05-04T10:34:12.191990Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 311ms/step - accuracy: 0.7399 - loss: 0.5230 - val_accuracy: 0.7771 - val_loss: 0.4671\nEpoch 2/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 313ms/step - accuracy: 0.7814 - loss: 0.4592 - val_accuracy: 0.7709 - val_loss: 0.4736\nEpoch 3/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 312ms/step - accuracy: 0.7906 - loss: 0.4416 - val_accuracy: 0.7784 - val_loss: 0.4626\nEpoch 4/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 317ms/step - accuracy: 0.7922 - loss: 0.4375 - val_accuracy: 0.7721 - val_loss: 0.4708\nEpoch 5/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m695s\u001b[0m 317ms/step - accuracy: 0.7951 - loss: 0.4328 - val_accuracy: 0.7846 - val_loss: 0.4560\nEpoch 6/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m775s\u001b[0m 353ms/step - accuracy: 0.7955 - loss: 0.4301 - val_accuracy: 0.7778 - val_loss: 0.4628\nEpoch 7/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 334ms/step - accuracy: 0.7984 - loss: 0.4277 - val_accuracy: 0.7839 - val_loss: 0.4543\nEpoch 8/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 313ms/step - accuracy: 0.7986 - loss: 0.4255 - val_accuracy: 0.7800 - val_loss: 0.4596\nEpoch 9/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 306ms/step - accuracy: 0.7979 - loss: 0.4274 - val_accuracy: 0.7833 - val_loss: 0.4555\nEpoch 10/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 316ms/step - accuracy: 0.7989 - loss: 0.4262 - val_accuracy: 0.7845 - val_loss: 0.4546\n\u001b[1m2974/2974\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m778s\u001b[0m 262ms/step - accuracy: 0.4159 - loss: 1.2127\nTest Loss: 1.2152163982391357, Test Accuracy: 0.41636061668395996\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EfficientNetV2M","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras as keras\nfrom tensorflow.keras.applications import EfficientNetV2M\nfrom tensorflow.keras.optimizers import Adam\n\n# Step 1: Load the EfficientNetV2-M model\nmodel = EfficientNetV2M(weights='imagenet', input_shape=(260, 260, 3), include_top=False)\n\n# Step 2: Freeze the convolutional base\nmodel.trainable = False\n\n# Step 3: Add custom classification head\nx = keras.layers.GlobalAveragePooling2D()(model.output)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n# Step 4: Compile the model\nmodel = keras.models.Model(inputs=model.input, outputs=outputs)\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Step 5: Train the model\nwith tf.device('/GPU:0'):\n    history = model.fit(train_data_v2M, epochs=10, validation_data=validation_data_v2M)\n\n# Step 6: Evaluate the model\nwith tf.device('/GPU:0'):\n    loss, accuracy = model.evaluate(test_data_v2M)\n    print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T10:34:12.194899Z","iopub.execute_input":"2024-05-04T10:34:12.195168Z","iopub.status.idle":"2024-05-04T12:54:17.989405Z","shell.execute_reply.started":"2024-05-04T10:34:12.195146Z","shell.execute_reply":"2024-05-04T12:54:17.986902Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 388ms/step - accuracy: 0.5064 - loss: 0.6946 - val_accuracy: 0.5021 - val_loss: 0.6924\nEpoch 2/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 348ms/step - accuracy: 0.5174 - loss: 0.6923 - val_accuracy: 0.5056 - val_loss: 0.6906\nEpoch 3/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m730s\u001b[0m 333ms/step - accuracy: 0.5268 - loss: 0.6911 - val_accuracy: 0.5344 - val_loss: 0.6889\nEpoch 4/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 329ms/step - accuracy: 0.5339 - loss: 0.6898 - val_accuracy: 0.4981 - val_loss: 0.6975\nEpoch 5/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m747s\u001b[0m 333ms/step - accuracy: 0.5390 - loss: 0.6882 - val_accuracy: 0.5351 - val_loss: 0.6872\nEpoch 7/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m724s\u001b[0m 330ms/step - accuracy: 0.5401 - loss: 0.6880 - val_accuracy: 0.5752 - val_loss: 0.6843\nEpoch 8/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 328ms/step - accuracy: 0.5452 - loss: 0.6868 - val_accuracy: 0.5506 - val_loss: 0.6854\nEpoch 9/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m727s\u001b[0m 332ms/step - accuracy: 0.5452 - loss: 0.6871 - val_accuracy: 0.5046 - val_loss: 0.6931\nEpoch 10/10\n\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 331ms/step - accuracy: 0.5471 - loss: 0.6867 - val_accuracy: 0.5783 - val_loss: 0.6827\n\u001b[1m2974/2974\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 252ms/step - accuracy: 0.3022 - loss: 0.7308\nTest Loss: 0.7308465838432312, Test Accuracy: 0.30323901772499084\n","output_type":"stream"}]}]}